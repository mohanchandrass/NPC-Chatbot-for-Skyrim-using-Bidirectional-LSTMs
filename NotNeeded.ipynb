{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PL1pH4Z_I_Nx",
        "outputId": "fac9eb37-4cee-40d1-d1a5-8eebab7ad79c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Vocabulary Size: 3594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7003 - loss: 3.1384✅ Tokenizer saved after epoch 1\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 216ms/step - accuracy: 0.7005 - loss: 3.1320 - val_accuracy: 0.7403 - val_loss: 1.9103 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.7390 - loss: 1.8978✅ Tokenizer saved after epoch 2\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.7390 - loss: 1.8977 - val_accuracy: 0.7411 - val_loss: 1.8682 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.7388 - loss: 1.8430✅ Tokenizer saved after epoch 3\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 158ms/step - accuracy: 0.7388 - loss: 1.8429 - val_accuracy: 0.7416 - val_loss: 1.8260 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.7394 - loss: 1.7905✅ Tokenizer saved after epoch 4\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 152ms/step - accuracy: 0.7394 - loss: 1.7903 - val_accuracy: 0.7431 - val_loss: 1.7853 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7428 - loss: 1.7199✅ Tokenizer saved after epoch 5\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 145ms/step - accuracy: 0.7428 - loss: 1.7198 - val_accuracy: 0.7428 - val_loss: 1.7606 - learning_rate: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7446 - loss: 1.6575✅ Tokenizer saved after epoch 6\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 154ms/step - accuracy: 0.7446 - loss: 1.6575 - val_accuracy: 0.7437 - val_loss: 1.7324 - learning_rate: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7457 - loss: 1.6075✅ Tokenizer saved after epoch 7\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 142ms/step - accuracy: 0.7457 - loss: 1.6075 - val_accuracy: 0.7455 - val_loss: 1.6974 - learning_rate: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7476 - loss: 1.5507✅ Tokenizer saved after epoch 8\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 149ms/step - accuracy: 0.7476 - loss: 1.5507 - val_accuracy: 0.7464 - val_loss: 1.6764 - learning_rate: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7508 - loss: 1.4894✅ Tokenizer saved after epoch 9\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 145ms/step - accuracy: 0.7507 - loss: 1.4895 - val_accuracy: 0.7476 - val_loss: 1.6472 - learning_rate: 0.0020\n",
            "Epoch 10/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7542 - loss: 1.4284✅ Tokenizer saved after epoch 10\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 146ms/step - accuracy: 0.7542 - loss: 1.4285 - val_accuracy: 0.7481 - val_loss: 1.6223 - learning_rate: 0.0020\n",
            "Epoch 11/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.7573 - loss: 1.3796✅ Tokenizer saved after epoch 11\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 146ms/step - accuracy: 0.7573 - loss: 1.3796 - val_accuracy: 0.7499 - val_loss: 1.6105 - learning_rate: 0.0020\n",
            "Epoch 12/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7618 - loss: 1.3272✅ Tokenizer saved after epoch 12\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 159ms/step - accuracy: 0.7618 - loss: 1.3272 - val_accuracy: 0.7510 - val_loss: 1.5927 - learning_rate: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7679 - loss: 1.2613✅ Tokenizer saved after epoch 13\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - accuracy: 0.7679 - loss: 1.2614 - val_accuracy: 0.7506 - val_loss: 1.5653 - learning_rate: 0.0020\n",
            "Epoch 14/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7684 - loss: 1.2321✅ Tokenizer saved after epoch 14\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 153ms/step - accuracy: 0.7684 - loss: 1.2320 - val_accuracy: 0.7526 - val_loss: 1.5479 - learning_rate: 0.0020\n",
            "Epoch 15/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7759 - loss: 1.1824✅ Tokenizer saved after epoch 15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.7759 - loss: 1.1824 - val_accuracy: 0.7553 - val_loss: 1.5324 - learning_rate: 0.0020\n",
            "Epoch 16/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.7803 - loss: 1.1344✅ Tokenizer saved after epoch 16\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.7803 - loss: 1.1344 - val_accuracy: 0.7563 - val_loss: 1.5206 - learning_rate: 0.0020\n",
            "Epoch 17/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7876 - loss: 1.0853✅ Tokenizer saved after epoch 17\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 143ms/step - accuracy: 0.7876 - loss: 1.0853 - val_accuracy: 0.7580 - val_loss: 1.5034 - learning_rate: 0.0020\n",
            "Epoch 18/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7920 - loss: 1.0561✅ Tokenizer saved after epoch 18\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 152ms/step - accuracy: 0.7920 - loss: 1.0561 - val_accuracy: 0.7597 - val_loss: 1.4944 - learning_rate: 0.0020\n",
            "Epoch 19/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7990 - loss: 1.0066✅ Tokenizer saved after epoch 19\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 153ms/step - accuracy: 0.7989 - loss: 1.0067 - val_accuracy: 0.7618 - val_loss: 1.4814 - learning_rate: 0.0020\n",
            "Epoch 20/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8055 - loss: 0.9788✅ Tokenizer saved after epoch 20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 153ms/step - accuracy: 0.8054 - loss: 0.9789 - val_accuracy: 0.7636 - val_loss: 1.4677 - learning_rate: 0.0020\n",
            "Epoch 21/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8099 - loss: 0.9548✅ Tokenizer saved after epoch 21\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 154ms/step - accuracy: 0.8099 - loss: 0.9548 - val_accuracy: 0.7661 - val_loss: 1.4598 - learning_rate: 0.0020\n",
            "Epoch 22/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8154 - loss: 0.9187✅ Tokenizer saved after epoch 22\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - accuracy: 0.8154 - loss: 0.9189 - val_accuracy: 0.7667 - val_loss: 1.4558 - learning_rate: 0.0020\n",
            "Epoch 23/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8165 - loss: 0.9049✅ Tokenizer saved after epoch 23\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 154ms/step - accuracy: 0.8165 - loss: 0.9049 - val_accuracy: 0.7699 - val_loss: 1.4530 - learning_rate: 0.0020\n",
            "Epoch 24/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.8226 - loss: 0.8699✅ Tokenizer saved after epoch 24\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 154ms/step - accuracy: 0.8225 - loss: 0.8700 - val_accuracy: 0.7702 - val_loss: 1.4455 - learning_rate: 0.0020\n",
            "Epoch 25/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8275 - loss: 0.8404✅ Tokenizer saved after epoch 25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 144ms/step - accuracy: 0.8274 - loss: 0.8406 - val_accuracy: 0.7699 - val_loss: 1.4428 - learning_rate: 0.0020\n",
            "Epoch 26/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8299 - loss: 0.8305✅ Tokenizer saved after epoch 26\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 150ms/step - accuracy: 0.8299 - loss: 0.8306 - val_accuracy: 0.7723 - val_loss: 1.4389 - learning_rate: 0.0020\n",
            "Epoch 27/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8307 - loss: 0.8224✅ Tokenizer saved after epoch 27\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - accuracy: 0.8307 - loss: 0.8224 - val_accuracy: 0.7721 - val_loss: 1.4293 - learning_rate: 0.0020\n",
            "Epoch 28/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8351 - loss: 0.8075✅ Tokenizer saved after epoch 28\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.8351 - loss: 0.8075 - val_accuracy: 0.7753 - val_loss: 1.4274 - learning_rate: 0.0020\n",
            "Epoch 29/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8403 - loss: 0.7750✅ Tokenizer saved after epoch 29\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 153ms/step - accuracy: 0.8403 - loss: 0.7752 - val_accuracy: 0.7754 - val_loss: 1.4257 - learning_rate: 0.0020\n",
            "Epoch 30/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8426 - loss: 0.7609✅ Tokenizer saved after epoch 30\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 150ms/step - accuracy: 0.8425 - loss: 0.7610 - val_accuracy: 0.7761 - val_loss: 1.4283 - learning_rate: 0.0020\n",
            "Epoch 31/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.8458 - loss: 0.7460✅ Tokenizer saved after epoch 31\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.8458 - loss: 0.7462 - val_accuracy: 0.7774 - val_loss: 1.4204 - learning_rate: 0.0020\n",
            "Epoch 32/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8445 - loss: 0.7519✅ Tokenizer saved after epoch 32\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 149ms/step - accuracy: 0.8445 - loss: 0.7519 - val_accuracy: 0.7794 - val_loss: 1.4249 - learning_rate: 0.0020\n",
            "Epoch 33/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8493 - loss: 0.7341✅ Tokenizer saved after epoch 33\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.8492 - loss: 0.7341 - val_accuracy: 0.7802 - val_loss: 1.4243 - learning_rate: 0.0020\n",
            "Epoch 34/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8518 - loss: 0.7061✅ Tokenizer saved after epoch 34\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.8518 - loss: 0.7063 - val_accuracy: 0.7819 - val_loss: 1.4236 - learning_rate: 0.0020\n",
            "Epoch 35/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8543 - loss: 0.6937✅ Tokenizer saved after epoch 35\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 152ms/step - accuracy: 0.8543 - loss: 0.6939 - val_accuracy: 0.7821 - val_loss: 1.4215 - learning_rate: 0.0020\n",
            "Epoch 36/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8599 - loss: 0.6853✅ Tokenizer saved after epoch 36\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 150ms/step - accuracy: 0.8599 - loss: 0.6852 - val_accuracy: 0.7839 - val_loss: 1.4144 - learning_rate: 4.0000e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8643 - loss: 0.6607✅ Tokenizer saved after epoch 37\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 140ms/step - accuracy: 0.8643 - loss: 0.6607 - val_accuracy: 0.7840 - val_loss: 1.4151 - learning_rate: 4.0000e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8661 - loss: 0.6517✅ Tokenizer saved after epoch 38\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 149ms/step - accuracy: 0.8661 - loss: 0.6518 - val_accuracy: 0.7848 - val_loss: 1.4169 - learning_rate: 4.0000e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.8673 - loss: 0.6472✅ Tokenizer saved after epoch 39\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 148ms/step - accuracy: 0.8673 - loss: 0.6472 - val_accuracy: 0.7849 - val_loss: 1.4175 - learning_rate: 4.0000e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8642 - loss: 0.6614✅ Tokenizer saved after epoch 40\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 155ms/step - accuracy: 0.8643 - loss: 0.6613 - val_accuracy: 0.7855 - val_loss: 1.4168 - learning_rate: 4.0000e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8689 - loss: 0.6414✅ Tokenizer saved after epoch 41\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.8689 - loss: 0.6414 - val_accuracy: 0.7858 - val_loss: 1.4176 - learning_rate: 1.0000e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8671 - loss: 0.6564✅ Tokenizer saved after epoch 42\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 149ms/step - accuracy: 0.8671 - loss: 0.6563 - val_accuracy: 0.7855 - val_loss: 1.4171 - learning_rate: 1.0000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8682 - loss: 0.6449✅ Tokenizer saved after epoch 43\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 147ms/step - accuracy: 0.8682 - loss: 0.6448 - val_accuracy: 0.7857 - val_loss: 1.4184 - learning_rate: 1.0000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8696 - loss: 0.6397✅ Tokenizer saved after epoch 44\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 149ms/step - accuracy: 0.8696 - loss: 0.6397 - val_accuracy: 0.7857 - val_loss: 1.4180 - learning_rate: 1.0000e-04\n",
            "✅ Model saved as /content/drive/MyDrive/chatbot/skyrim_chatbot_final.keras\n",
            "Welcome to the Skyrim Chatbot! Type 'exit' to leave.\n",
            "You: HELLO\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-706784b43899>:116: RuntimeWarning: divide by zero encountered in log\n",
            "  predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "probabilities do not sum to 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-706784b43899>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NPC: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-706784b43899>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(player_input, model, tokenizer, max_sequence_length, temperature)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mpredicted_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mpredicted_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-706784b43899>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mpredicted_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mpredicted_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/skyrim_dialogue_dataset_10000V2.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_len = 50\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_padded = pad_sequences(input_sequences, maxlen=max_len, padding='post')\n",
        "response_padded = pad_sequences(response_sequences, maxlen=max_len, padding='post')\n",
        "# **🧪 Train-Test Split**\n",
        "X_train, X_val, y_train, y_val = train_test_split( input_padded, response_padded, test_size=0.2, random_state=42)\n",
        "# **🛠️ LSTM Model Architecture**\n",
        "embedding_dim = 256  # Increased embedding dimension\n",
        "lstm_units = 512  # Increased LSTM units\n",
        "dropout_rate = 0.3\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "    Dropout(dropout_rate),\n",
        "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# **⚙️ Compile Model**\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# **📌 Custom Callback to Save Tokenizer**\n",
        "class SaveTokenizerCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ Tokenizer saved after epoch {epoch + 1}\")\n",
        "\n",
        "# **🛡️ Callbacks**\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001),\n",
        "    SaveTokenizerCallback()\n",
        "]\n",
        "\n",
        "# **🎯 Train Model**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **✅ Save Final Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Model saved as {final_model_path}\")\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.8):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = [np.random.choice(len(seq), p=seq) for seq in predicted_seq]\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([sampled_indices])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEMXqpfWNoka",
        "outputId": "aa052f47-cf8c-43cb-c7cf-a643c6c0adcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Model loaded from /content/drive/MyDrive/chatbot/skyrim_chatbot_best.keras\n",
            "✅ Tokenizer loaded from /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "✅ Vocabulary Size: 3591\n",
            "Epoch 1/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8535 - loss: 0.5860✅ Epoch 1: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 290ms/step - accuracy: 0.8533 - loss: 0.5868 - val_accuracy: 0.7412 - val_loss: 2.1781 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8569 - loss: 0.5620✅ Epoch 2: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 251ms/step - accuracy: 0.8568 - loss: 0.5625 - val_accuracy: 0.7379 - val_loss: 2.1798 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.8666 - loss: 0.5278✅ Epoch 3: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 221ms/step - accuracy: 0.8665 - loss: 0.5282 - val_accuracy: 0.7388 - val_loss: 2.2090 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.8687 - loss: 0.5146✅ Epoch 4: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 209ms/step - accuracy: 0.8687 - loss: 0.5148 - val_accuracy: 0.7394 - val_loss: 2.2250 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8791 - loss: 0.4719✅ Epoch 5: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 227ms/step - accuracy: 0.8791 - loss: 0.4721 - val_accuracy: 0.7402 - val_loss: 2.2208 - learning_rate: 4.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.8769 - loss: 0.4785✅ Epoch 6: Model & Tokenizer saved!\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 252ms/step - accuracy: 0.8770 - loss: 0.4783 - val_accuracy: 0.7401 - val_loss: 2.2219 - learning_rate: 4.0000e-04\n",
            "✅ Retraining complete! Model & Tokenizer saved after every epoch.\n",
            "✅ Vocabulary Size: 3591\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive  # Mount Google Drive\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# **📁 Define Google Drive Save Path**\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "updated_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "\n",
        "# **📌 Load the Saved Model**\n",
        "if os.path.exists(best_model_path):\n",
        "    model = tf.keras.models.load_model(best_model_path)\n",
        "    print(f\"✅ Model loaded from {best_model_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Model file not found at {best_model_path}\")\n",
        "\n",
        "# **📌 Load Tokenizer**\n",
        "if os.path.exists(tokenizer_path):\n",
        "    with open(tokenizer_path, \"rb\") as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    print(f\"✅ Tokenizer loaded from {tokenizer_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Tokenizer file not found at {tokenizer_path}\")\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    return text.strip()\n",
        "\n",
        "# **📂 Load Skyrim Dialogue Dataset**\n",
        "file_path = \"/content/drive/MyDrive/chatbot/dataset2796.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "\n",
        "# **Tokenize & Pad Sequences**\n",
        "max_sequence_length = 30  # Same as previous training\n",
        "\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# **Convert responses to numpy array for training**\n",
        "y_train = np.array(response_sequences)\n",
        "\n",
        "# **🧪 Split dataset into train & test**\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# **⚙️ Recompile Model with Optimizer & Loss**\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# **🛡️ Custom Callback to Save Model & Tokenizer After Each Epoch**\n",
        "class SaveModelAndTokenizer(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save model\n",
        "        model.save(updated_model_path)\n",
        "\n",
        "        # Save tokenizer\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        print(f\"✅ Epoch {epoch+1}: Model & Tokenizer saved!\")\n",
        "\n",
        "# **🛡️ Callbacks for Training**\n",
        "callbacks = [\n",
        "    SaveModelAndTokenizer(),  # Custom callback to save both model & tokenizer\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "# **🎯 Retrain Model**\n",
        "try:\n",
        "    epochs = 50  # Additional training\n",
        "    batch_size = 64\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Retraining complete! Model & Tokenizer saved after every epoch.\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️ Training interrupted! Saving latest model before exiting...\")\n",
        "    model.save(os.path.join(drive_save_path, \"skyrim_chatbot_latest.keras\"))\n",
        "    print(f\"✅ Latest model saved as skyrim_chatbot_latest.keras\")\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S5D-VDghQIAX",
        "outputId": "22bfca4f-6d58-4a40-94de-20cb371feaf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.002, 'decay_steps': 100000, 'decay_rate': 0.96, 'staircase': True, 'name': 'ExponentialDecay'}, 'registered_name': None}, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': 'sparse_categorical_crossentropy', 'loss_weights': None, 'metrics': ['accuracy'], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.\n\nException encountered: Could not locate class 'BahdanauAttention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'BahdanauAttention', 'config': {'units': 512, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'mixed_float16'}, 'registered_name': None, 'shared_object_id': 133884838400592}}, 'registered_name': 'BahdanauAttention', 'build_config': {'input_shape': [None, 512]}, 'name': 'bahdanau_attention_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 512], 'dtype': 'float16', 'keras_history': ['lstm_5', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 30, 1024], 'dtype': 'float16', 'keras_history': ['bidirectional_2', 0, 0]}}], 'kwargs': {}}]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'BahdanauAttention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'BahdanauAttention', 'config': {'units': 512, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'mixed_float16'}, 'registered_name': None, 'shared_object_id': 133884838400592}}, 'registered_name': 'BahdanauAttention', 'build_config': {'input_shape': [None, 512]}, 'name': 'bahdanau_attention_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 512], 'dtype': 'float16', 'keras_history': ['lstm_5', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 30, 1024], 'dtype': 'float16', 'keras_history': ['bidirectional_2', 0, 0]}}], 'kwargs': {}}]}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4711b5fac278>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# **📌 Load the Updated Model**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ Model loaded from {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'ExponentialDecay', 'config': {'initial_learning_rate': 0.002, 'decay_steps': 100000, 'decay_rate': 0.96, 'staircase': True, 'name': 'ExponentialDecay'}, 'registered_name': None}, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': 'sparse_categorical_crossentropy', 'loss_weights': None, 'metrics': ['accuracy'], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.\n\nException encountered: Could not locate class 'BahdanauAttention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'BahdanauAttention', 'config': {'units': 512, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'mixed_float16'}, 'registered_name': None, 'shared_object_id': 133884838400592}}, 'registered_name': 'BahdanauAttention', 'build_config': {'input_shape': [None, 512]}, 'name': 'bahdanau_attention_2', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 512], 'dtype': 'float16', 'keras_history': ['lstm_5', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 30, 1024], 'dtype': 'float16', 'keras_history': ['bidirectional_2', 0, 0]}}], 'kwargs': {}}]}"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive  # Mount Google Drive\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# **📁 Define Google Drive Path**\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "model_path = os.path.join(drive_save_path, \"skyrim_chatbot_ea.keras\")\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "\n",
        "# **📌 Load the Updated Model**\n",
        "if os.path.exists(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    print(f\"✅ Model loaded from {model_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Model file not found at {model_path}\")\n",
        "\n",
        "# **📌 Load the Tokenizer**\n",
        "if os.path.exists(tokenizer_path):\n",
        "    with open(tokenizer_path, \"rb\") as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    print(f\"✅ Tokenizer loaded from {tokenizer_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Tokenizer file not found at {tokenizer_path}\")\n",
        "\n",
        "# **Response Generation Function (Fixed)**\n",
        "def generate_response(input_text, max_response_length=30, temperature=0.2, top_k=5):\n",
        "    max_sequence_length = 30  # Must match training\n",
        "\n",
        "    # **Tokenize and pad the input text**\n",
        "    input_seq = pad_sequences(tokenizer.texts_to_sequences([input_text]), maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # **Predict the response sequence**\n",
        "    prediction = model.predict(input_seq, verbose=0)  # Shape: (1, max_sequence_length, vocab_size)\n",
        "\n",
        "    # **Ensure response generation starts correctly**\n",
        "    response_text = []\n",
        "    previous_words = set()  # Track previous words to avoid repetition\n",
        "\n",
        "    for i in range(max_response_length):\n",
        "        word_probs = prediction[0, i]  # Extract probabilities for the i-th word\n",
        "\n",
        "        # **Temperature scaling for controlled randomness**\n",
        "        word_probs = np.exp(word_probs / temperature)\n",
        "        word_probs /= np.sum(word_probs)  # Normalize probabilities\n",
        "\n",
        "        # **Select the top-k words (Beam Search)**\n",
        "        top_indices = np.argsort(word_probs)[-top_k:]  # Get top-k indices\n",
        "        word_idx = np.random.choice(top_indices, p=word_probs[top_indices] / np.sum(word_probs[top_indices]))\n",
        "\n",
        "        # **Handle unknown tokens & repetition**\n",
        "        if word_idx not in tokenizer.index_word:\n",
        "            continue  # Skip unrecognized words\n",
        "\n",
        "        word = tokenizer.index_word[word_idx]\n",
        "\n",
        "        if word in previous_words or word in [\"<OOV>\", \"<UNK>\", \"<PAD>\", \"<END>\"]:\n",
        "            continue  # Prevents repeating the same word or invalid tokens\n",
        "\n",
        "        response_text.append(word)\n",
        "        previous_words.add(word)  # Track words used\n",
        "\n",
        "        # **Break if a complete response is formed**\n",
        "        if word in [\".\", \"?\", \"!\"]:\n",
        "            break\n",
        "\n",
        "    # **Format the final response**\n",
        "    response = \" \".join(response_text).strip()\n",
        "\n",
        "    # **Fallback Response if Model Fails**\n",
        "    if not response:\n",
        "        fallback_responses = [\n",
        "            \"I am not sure about that traveler, but perhaps the answer lies elsewhere.\",\n",
        "            \"That is a question for the wise, not a mere wanderer like me.\",\n",
        "            \"Perhaps the Divines have the answer you seek.\",\n",
        "            \"That is a tale lost to time, traveler.\"\n",
        "        ]\n",
        "        response = np.random.choice(fallback_responses)  # Random fallback for variety\n",
        "\n",
        "    return response\n",
        "\n",
        "# **Chatbot interaction loop**\n",
        "try:\n",
        "    print(\"Welcome to the Elder Scrolls NPC Chatbot! Type 'exit' or 'quit' to end the conversation.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "        # **Exit conditions**\n",
        "        if user_input in [\"exit\", \"quit\"]:\n",
        "            print(\"NPC: Farewell, traveler. May the blessings of the Tribunal be with you!\")\n",
        "            break\n",
        "\n",
        "        # **Generate and display NPC response**\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"NPC: {response}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nNPC: Farewell, traveler. May the blessings of the Tribunal be with you!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OiDyiET5Qpg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "RDleYBtiSdrC",
        "outputId": "dd80446b-00da-421a-8282-35ac2c36ee5d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f389158adf91>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# **🔗 Mount Google Drive**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdrive_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/chatbot/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zAZ0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/skyrim_dialogue_dataset_10000V2.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_sequence_length = 30\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_train = np.array(response_sequences, dtype=np.int32)\n",
        "\n",
        "# **🧪 Train-Test Split**\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# **🛠️ LSTM Model Architecture with Attention Layer**\n",
        "embedding_dim = 256\n",
        "lstm_units = 512  # Increased LSTM units for more capacity\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
        "    tf.keras.layers.Attention(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax', dtype='float32'))\n",
        "])\n",
        "\n",
        "# **⚙️ Compile Model with Learning Rate Schedule**\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=0.002, decay_steps=100000, decay_rate=0.96, staircase=True)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# **📌 Custom Callback to Save Tokenizer**\n",
        "class SaveTokenizerCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ Tokenizer saved after epoch {epoch + 1}\")\n",
        "\n",
        "# **🛡️ Callbacks**\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001),\n",
        "    SaveTokenizerCallback()\n",
        "]\n",
        "\n",
        "# **🎯 Train Model**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # Increased epochs for better training\n",
        "    batch_size=128,  # Larger batch size for stability\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **✅ Save Final Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Model saved as {final_model_path}\")\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.8):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = [np.random.choice(len(seq), p=seq) for seq in predicted_seq]\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([sampled_indices])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQV25RGy5Rf5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OvxBmLTNzOsK",
        "outputId": "3cb60e94-536d-4fda-8626-102c67515c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Vocabulary Size: 3593\n",
            "Epoch 1/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.6792 - loss: 3.4495✅ Tokenizer saved after epoch 1\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 254ms/step - accuracy: 0.6798 - loss: 3.4364 - val_accuracy: 0.7345 - val_loss: 2.0273 - learning_rate: 0.0020\n",
            "Epoch 2/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.7350 - loss: 1.9882✅ Tokenizer saved after epoch 2\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 250ms/step - accuracy: 0.7351 - loss: 1.9879 - val_accuracy: 0.7403 - val_loss: 1.9139 - learning_rate: 0.0020\n",
            "Epoch 3/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7403 - loss: 1.8984✅ Tokenizer saved after epoch 3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 251ms/step - accuracy: 0.7403 - loss: 1.8983 - val_accuracy: 0.7410 - val_loss: 1.8676 - learning_rate: 0.0020\n",
            "Epoch 4/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7385 - loss: 1.8656✅ Tokenizer saved after epoch 4\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 255ms/step - accuracy: 0.7385 - loss: 1.8653 - val_accuracy: 0.7415 - val_loss: 1.8421 - learning_rate: 0.0020\n",
            "Epoch 5/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7397 - loss: 1.8183✅ Tokenizer saved after epoch 5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 259ms/step - accuracy: 0.7398 - loss: 1.8182 - val_accuracy: 0.7415 - val_loss: 1.8188 - learning_rate: 0.0020\n",
            "Epoch 6/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7439 - loss: 1.7692✅ Tokenizer saved after epoch 6\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 247ms/step - accuracy: 0.7439 - loss: 1.7695 - val_accuracy: 0.7419 - val_loss: 1.8049 - learning_rate: 0.0020\n",
            "Epoch 7/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7404 - loss: 1.7681✅ Tokenizer saved after epoch 7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 258ms/step - accuracy: 0.7404 - loss: 1.7680 - val_accuracy: 0.7426 - val_loss: 1.7824 - learning_rate: 0.0020\n",
            "Epoch 8/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7408 - loss: 1.7419✅ Tokenizer saved after epoch 8\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - accuracy: 0.7409 - loss: 1.7418 - val_accuracy: 0.7434 - val_loss: 1.7677 - learning_rate: 0.0020\n",
            "Epoch 9/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7445 - loss: 1.6961✅ Tokenizer saved after epoch 9\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 254ms/step - accuracy: 0.7445 - loss: 1.6962 - val_accuracy: 0.7438 - val_loss: 1.7350 - learning_rate: 0.0020\n",
            "Epoch 10/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7451 - loss: 1.6603✅ Tokenizer saved after epoch 10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 246ms/step - accuracy: 0.7451 - loss: 1.6604 - val_accuracy: 0.7446 - val_loss: 1.7109 - learning_rate: 0.0020\n",
            "Epoch 11/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7448 - loss: 1.6313✅ Tokenizer saved after epoch 11\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 247ms/step - accuracy: 0.7448 - loss: 1.6313 - val_accuracy: 0.7455 - val_loss: 1.6943 - learning_rate: 0.0020\n",
            "Epoch 12/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7459 - loss: 1.6003✅ Tokenizer saved after epoch 12\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 243ms/step - accuracy: 0.7459 - loss: 1.6004 - val_accuracy: 0.7459 - val_loss: 1.6729 - learning_rate: 0.0020\n",
            "Epoch 13/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7433 - loss: 1.5870✅ Tokenizer saved after epoch 13\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 264ms/step - accuracy: 0.7434 - loss: 1.5867 - val_accuracy: 0.7460 - val_loss: 1.6546 - learning_rate: 0.0020\n",
            "Epoch 14/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.7472 - loss: 1.5366✅ Tokenizer saved after epoch 14\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 242ms/step - accuracy: 0.7472 - loss: 1.5366 - val_accuracy: 0.7464 - val_loss: 1.6318 - learning_rate: 0.0020\n",
            "Epoch 15/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7523 - loss: 1.4851✅ Tokenizer saved after epoch 15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 262ms/step - accuracy: 0.7523 - loss: 1.4853 - val_accuracy: 0.7479 - val_loss: 1.6112 - learning_rate: 0.0020\n",
            "Epoch 16/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.7515 - loss: 1.4649✅ Tokenizer saved after epoch 16\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 249ms/step - accuracy: 0.7515 - loss: 1.4650 - val_accuracy: 0.7492 - val_loss: 1.5920 - learning_rate: 0.0020\n",
            "Epoch 17/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7546 - loss: 1.4185✅ Tokenizer saved after epoch 17\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 243ms/step - accuracy: 0.7545 - loss: 1.4188 - val_accuracy: 0.7510 - val_loss: 1.5676 - learning_rate: 0.0020\n",
            "Epoch 18/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7560 - loss: 1.3941✅ Tokenizer saved after epoch 18\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 259ms/step - accuracy: 0.7560 - loss: 1.3941 - val_accuracy: 0.7530 - val_loss: 1.5480 - learning_rate: 0.0020\n",
            "Epoch 19/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7602 - loss: 1.3489✅ Tokenizer saved after epoch 19\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 243ms/step - accuracy: 0.7602 - loss: 1.3491 - val_accuracy: 0.7546 - val_loss: 1.5297 - learning_rate: 0.0020\n",
            "Epoch 20/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7627 - loss: 1.3225✅ Tokenizer saved after epoch 20\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 250ms/step - accuracy: 0.7627 - loss: 1.3225 - val_accuracy: 0.7562 - val_loss: 1.5113 - learning_rate: 0.0020\n",
            "Epoch 21/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7668 - loss: 1.2839✅ Tokenizer saved after epoch 21\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 251ms/step - accuracy: 0.7668 - loss: 1.2841 - val_accuracy: 0.7573 - val_loss: 1.4945 - learning_rate: 0.0020\n",
            "Epoch 22/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7684 - loss: 1.2575✅ Tokenizer saved after epoch 22\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 246ms/step - accuracy: 0.7684 - loss: 1.2576 - val_accuracy: 0.7584 - val_loss: 1.4709 - learning_rate: 0.0020\n",
            "Epoch 23/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.7725 - loss: 1.2212✅ Tokenizer saved after epoch 23\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 247ms/step - accuracy: 0.7725 - loss: 1.2213 - val_accuracy: 0.7608 - val_loss: 1.4559 - learning_rate: 0.0020\n",
            "Epoch 24/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.7769 - loss: 1.1970✅ Tokenizer saved after epoch 24\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 243ms/step - accuracy: 0.7769 - loss: 1.1971 - val_accuracy: 0.7629 - val_loss: 1.4389 - learning_rate: 0.0020\n",
            "Epoch 25/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7787 - loss: 1.1746✅ Tokenizer saved after epoch 25\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - accuracy: 0.7787 - loss: 1.1746 - val_accuracy: 0.7653 - val_loss: 1.4193 - learning_rate: 0.0020\n",
            "Epoch 26/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.7834 - loss: 1.1421✅ Tokenizer saved after epoch 26\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 252ms/step - accuracy: 0.7834 - loss: 1.1421 - val_accuracy: 0.7675 - val_loss: 1.4088 - learning_rate: 0.0020\n",
            "Epoch 27/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.7865 - loss: 1.1162✅ Tokenizer saved after epoch 27\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 262ms/step - accuracy: 0.7865 - loss: 1.1162 - val_accuracy: 0.7680 - val_loss: 1.3995 - learning_rate: 0.0020\n",
            "Epoch 28/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.7918 - loss: 1.0821✅ Tokenizer saved after epoch 28\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 242ms/step - accuracy: 0.7917 - loss: 1.0823 - val_accuracy: 0.7689 - val_loss: 1.3848 - learning_rate: 0.0020\n",
            "Epoch 29/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7926 - loss: 1.0663✅ Tokenizer saved after epoch 29\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 252ms/step - accuracy: 0.7926 - loss: 1.0664 - val_accuracy: 0.7721 - val_loss: 1.3654 - learning_rate: 0.0020\n",
            "Epoch 30/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7968 - loss: 1.0409✅ Tokenizer saved after epoch 30\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 239ms/step - accuracy: 0.7968 - loss: 1.0411 - val_accuracy: 0.7728 - val_loss: 1.3659 - learning_rate: 0.0020\n",
            "Epoch 31/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.7997 - loss: 1.0250✅ Tokenizer saved after epoch 31\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 240ms/step - accuracy: 0.7997 - loss: 1.0251 - val_accuracy: 0.7751 - val_loss: 1.3451 - learning_rate: 0.0020\n",
            "Epoch 32/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8052 - loss: 0.9967✅ Tokenizer saved after epoch 32\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 242ms/step - accuracy: 0.8051 - loss: 0.9969 - val_accuracy: 0.7772 - val_loss: 1.3372 - learning_rate: 0.0020\n",
            "Epoch 33/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8051 - loss: 0.9851✅ Tokenizer saved after epoch 33\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 262ms/step - accuracy: 0.8051 - loss: 0.9852 - val_accuracy: 0.7782 - val_loss: 1.3291 - learning_rate: 0.0020\n",
            "Epoch 34/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8071 - loss: 0.9747✅ Tokenizer saved after epoch 34\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 243ms/step - accuracy: 0.8070 - loss: 0.9746 - val_accuracy: 0.7806 - val_loss: 1.3186 - learning_rate: 0.0020\n",
            "Epoch 35/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8106 - loss: 0.9463✅ Tokenizer saved after epoch 35\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 251ms/step - accuracy: 0.8106 - loss: 0.9464 - val_accuracy: 0.7813 - val_loss: 1.3068 - learning_rate: 0.0020\n",
            "Epoch 36/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8147 - loss: 0.9250✅ Tokenizer saved after epoch 36\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.8147 - loss: 0.9252 - val_accuracy: 0.7825 - val_loss: 1.3071 - learning_rate: 0.0020\n",
            "Epoch 37/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8179 - loss: 0.9017✅ Tokenizer saved after epoch 37\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 232ms/step - accuracy: 0.8179 - loss: 0.9020 - val_accuracy: 0.7845 - val_loss: 1.2958 - learning_rate: 0.0020\n",
            "Epoch 38/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8194 - loss: 0.8980✅ Tokenizer saved after epoch 38\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 247ms/step - accuracy: 0.8194 - loss: 0.8982 - val_accuracy: 0.7854 - val_loss: 1.2915 - learning_rate: 0.0020\n",
            "Epoch 39/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8213 - loss: 0.8902✅ Tokenizer saved after epoch 39\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 242ms/step - accuracy: 0.8213 - loss: 0.8903 - val_accuracy: 0.7876 - val_loss: 1.2847 - learning_rate: 0.0020\n",
            "Epoch 40/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8244 - loss: 0.8702✅ Tokenizer saved after epoch 40\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 234ms/step - accuracy: 0.8243 - loss: 0.8704 - val_accuracy: 0.7879 - val_loss: 1.2847 - learning_rate: 0.0020\n",
            "Epoch 41/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8256 - loss: 0.8618✅ Tokenizer saved after epoch 41\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 235ms/step - accuracy: 0.8256 - loss: 0.8620 - val_accuracy: 0.7888 - val_loss: 1.2752 - learning_rate: 0.0020\n",
            "Epoch 42/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8260 - loss: 0.8511✅ Tokenizer saved after epoch 42\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 231ms/step - accuracy: 0.8259 - loss: 0.8512 - val_accuracy: 0.7904 - val_loss: 1.2772 - learning_rate: 0.0020\n",
            "Epoch 43/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8297 - loss: 0.8354✅ Tokenizer saved after epoch 43\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 242ms/step - accuracy: 0.8297 - loss: 0.8356 - val_accuracy: 0.7910 - val_loss: 1.2694 - learning_rate: 0.0020\n",
            "Epoch 44/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8285 - loss: 0.8465✅ Tokenizer saved after epoch 44\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 241ms/step - accuracy: 0.8285 - loss: 0.8463 - val_accuracy: 0.7916 - val_loss: 1.2634 - learning_rate: 0.0020\n",
            "Epoch 45/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8315 - loss: 0.8295✅ Tokenizer saved after epoch 45\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 240ms/step - accuracy: 0.8315 - loss: 0.8295 - val_accuracy: 0.7928 - val_loss: 1.2618 - learning_rate: 0.0020\n",
            "Epoch 46/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8350 - loss: 0.8030✅ Tokenizer saved after epoch 46\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 230ms/step - accuracy: 0.8350 - loss: 0.8033 - val_accuracy: 0.7938 - val_loss: 1.2647 - learning_rate: 0.0020\n",
            "Epoch 47/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.8396 - loss: 0.7829✅ Tokenizer saved after epoch 47\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 242ms/step - accuracy: 0.8395 - loss: 0.7833 - val_accuracy: 0.7945 - val_loss: 1.2562 - learning_rate: 0.0020\n",
            "Epoch 48/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8380 - loss: 0.7857✅ Tokenizer saved after epoch 48\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 228ms/step - accuracy: 0.8380 - loss: 0.7859 - val_accuracy: 0.7952 - val_loss: 1.2570 - learning_rate: 0.0020\n",
            "Epoch 49/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8386 - loss: 0.7860✅ Tokenizer saved after epoch 49\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.8386 - loss: 0.7861 - val_accuracy: 0.7963 - val_loss: 1.2482 - learning_rate: 0.0020\n",
            "Epoch 50/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8416 - loss: 0.7750✅ Tokenizer saved after epoch 50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 238ms/step - accuracy: 0.8416 - loss: 0.7751 - val_accuracy: 0.7973 - val_loss: 1.2498 - learning_rate: 0.0020\n",
            "Epoch 51/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8433 - loss: 0.7630✅ Tokenizer saved after epoch 51\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 221ms/step - accuracy: 0.8433 - loss: 0.7632 - val_accuracy: 0.7984 - val_loss: 1.2504 - learning_rate: 0.0020\n",
            "Epoch 52/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8420 - loss: 0.7630✅ Tokenizer saved after epoch 52\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 243ms/step - accuracy: 0.8420 - loss: 0.7631 - val_accuracy: 0.7989 - val_loss: 1.2437 - learning_rate: 0.0020\n",
            "Epoch 53/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8433 - loss: 0.7505✅ Tokenizer saved after epoch 53\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.8433 - loss: 0.7507 - val_accuracy: 0.7996 - val_loss: 1.2480 - learning_rate: 0.0020\n",
            "Epoch 54/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8437 - loss: 0.7594✅ Tokenizer saved after epoch 54\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.8437 - loss: 0.7593 - val_accuracy: 0.8001 - val_loss: 1.2407 - learning_rate: 0.0020\n",
            "Epoch 55/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8492 - loss: 0.7354✅ Tokenizer saved after epoch 55\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 230ms/step - accuracy: 0.8492 - loss: 0.7356 - val_accuracy: 0.8009 - val_loss: 1.2422 - learning_rate: 0.0020\n",
            "Epoch 56/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.8488 - loss: 0.7317✅ Tokenizer saved after epoch 56\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 241ms/step - accuracy: 0.8488 - loss: 0.7318 - val_accuracy: 0.8016 - val_loss: 1.2403 - learning_rate: 0.0020\n",
            "Epoch 57/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8501 - loss: 0.7195✅ Tokenizer saved after epoch 57\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 255ms/step - accuracy: 0.8500 - loss: 0.7197 - val_accuracy: 0.8013 - val_loss: 1.2373 - learning_rate: 0.0020\n",
            "Epoch 58/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.8511 - loss: 0.7206✅ Tokenizer saved after epoch 58\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.8511 - loss: 0.7207 - val_accuracy: 0.8027 - val_loss: 1.2374 - learning_rate: 0.0020\n",
            "Epoch 59/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8513 - loss: 0.7122✅ Tokenizer saved after epoch 59\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 219ms/step - accuracy: 0.8513 - loss: 0.7124 - val_accuracy: 0.8036 - val_loss: 1.2416 - learning_rate: 0.0020\n",
            "Epoch 60/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.8543 - loss: 0.7040✅ Tokenizer saved after epoch 60\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 222ms/step - accuracy: 0.8543 - loss: 0.7042 - val_accuracy: 0.8030 - val_loss: 1.2422 - learning_rate: 0.0020\n",
            "Epoch 61/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8535 - loss: 0.7080"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c9542a45c768>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# **🎯 Train Model**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Increased epochs for better training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mlearning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             ):\n\u001b[0;32m--> 636\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    637\u001b[0m                     \u001b[0;34m\"This optimizer was created with a `LearningRateSchedule`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                     \u001b[0;34m\" object as its `learning_rate` constructor argument, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zAZ0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/skyrim_dialogue_dataset_10000V2.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_sequence_length = 30\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_train = np.array(response_sequences, dtype=np.int32)\n",
        "\n",
        "# **🧪 Train-Test Split**\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# **🛠️ LSTM Model Architecture with Attention Layer**\n",
        "embedding_dim = 256\n",
        "lstm_units = 512  # Increased LSTM units for more capacity\n",
        "\n",
        "# LSTM Model with Attention\n",
        "inputs = tf.keras.layers.Input(shape=(max_len,))\n",
        "embed = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256)(inputs)\n",
        "lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True, dropout=0.3))(embed)\n",
        "attention = tf.keras.layers.Attention()([lstm, lstm])\n",
        "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax', dtype='float32'))(attention)\n",
        "\n",
        "model = tf.keras.models.Model(inputs, output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(0.002, 100000, 0.96, staircase=True)),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint(os.path.join(save_path, \"best_model.keras\"), save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001)\n",
        "])\n",
        "\n",
        "\n",
        "# **📌 Custom Callback to Save Tokenizer**\n",
        "class SaveTokenizerCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ Tokenizer saved after epoch {epoch + 1}\")\n",
        "\n",
        "# **🛡️ Callbacks**\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001),\n",
        "    SaveTokenizerCallback()\n",
        "]\n",
        "\n",
        "# **🎯 Train Model**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # Increased epochs for better training\n",
        "    batch_size=128,  # Larger batch size for stability\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **✅ Save Final Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Model saved as {final_model_path}\")\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.3):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = [np.random.choice(len(seq), p=seq) for seq in predicted_seq]\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([sampled_indices])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmfEtddB5SxP",
        "outputId": "d508587c-87ab-4ad5-9d40-db2d3504cf37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8365 - loss: 0.8239✅ Tokenizer saved after epoch 1\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 265ms/step - accuracy: 0.8364 - loss: 0.8243 - val_accuracy: 0.8464 - val_loss: 0.8135\n",
            "Epoch 2/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8349 - loss: 0.8327✅ Tokenizer saved after epoch 2\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 257ms/step - accuracy: 0.8349 - loss: 0.8327 - val_accuracy: 0.8480 - val_loss: 0.8131\n",
            "Epoch 3/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.8379 - loss: 0.8120✅ Tokenizer saved after epoch 3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 248ms/step - accuracy: 0.8379 - loss: 0.8121 - val_accuracy: 0.8475 - val_loss: 0.8155\n",
            "Epoch 4/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8419 - loss: 0.7929✅ Tokenizer saved after epoch 4\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 235ms/step - accuracy: 0.8418 - loss: 0.7930 - val_accuracy: 0.8475 - val_loss: 0.8184\n",
            "Epoch 5/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8441 - loss: 0.7862✅ Tokenizer saved after epoch 5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 228ms/step - accuracy: 0.8441 - loss: 0.7862 - val_accuracy: 0.8476 - val_loss: 0.8205\n",
            "Epoch 6/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8456 - loss: 0.7700✅ Tokenizer saved after epoch 6\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 236ms/step - accuracy: 0.8456 - loss: 0.7701 - val_accuracy: 0.8468 - val_loss: 0.8263\n",
            "Epoch 7/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8474 - loss: 0.7590✅ Tokenizer saved after epoch 7\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 233ms/step - accuracy: 0.8474 - loss: 0.7592 - val_accuracy: 0.8468 - val_loss: 0.8273\n",
            "Epoch 8/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8498 - loss: 0.7474✅ Tokenizer saved after epoch 8\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 235ms/step - accuracy: 0.8497 - loss: 0.7475 - val_accuracy: 0.8464 - val_loss: 0.8338\n",
            "Epoch 9/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.8510 - loss: 0.7312✅ Tokenizer saved after epoch 9\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 238ms/step - accuracy: 0.8510 - loss: 0.7315 - val_accuracy: 0.8460 - val_loss: 0.8354\n",
            "Epoch 10/50\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8502 - loss: 0.7405✅ Tokenizer saved after epoch 10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 228ms/step - accuracy: 0.8502 - loss: 0.7405 - val_accuracy: 0.8455 - val_loss: 0.8435\n",
            "✅ Retrained Model saved as /content/drive/MyDrive/chatbot/skyrim_chatbot_best.keras\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "\n",
        "# **📌 Load Tokenizer**\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"rb\") as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "max_sequence_length = 30  # Ensure it matches the original model\n",
        "\n",
        "# **📌 Load Saved Model**\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "model = tf.keras.models.load_model(best_model_path)\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    import pandas as pd\n",
        "    import re\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "        return text.strip()\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/skyrim_dialogue_dataset_10000v1.1.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Convert Text Data to Sequences**\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_train = np.array(response_sequences, dtype=np.int32)\n",
        "\n",
        "# **🧪 Train-Test Split**\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# **📌 Callbacks for Training**\n",
        "class SaveTokenizerCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ Tokenizer saved after epoch {epoch + 1}\")\n",
        "\n",
        "# **Recompile Model Without Resetting Learning Rate**\n",
        "model.compile(optimizer=model.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True),\n",
        "    SaveTokenizerCallback()\n",
        "]\n",
        "\n",
        "# **🎯 Continue Training**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **✅ Save Updated Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Retrained Model saved as {final_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF4SBkz_uppZ",
        "outputId": "5b823bc1-6c04-4dd4-c8da-b381792909dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Vocabulary Size: 3665\n",
            "Epoch 1/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.6968 - loss: 2.9926✅ Tokenizer saved after epoch 1\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 195ms/step - accuracy: 0.6969 - loss: 2.9883 - learning_rate: 0.0020\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:209: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py:145: UserWarning: Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss,learning_rate.\n",
            "  callback.on_epoch_end(epoch, logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.7285 - loss: 1.9693✅ Tokenizer saved after epoch 2\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 204ms/step - accuracy: 0.7286 - loss: 1.9693 - learning_rate: 0.0020\n",
            "Epoch 3/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7305 - loss: 1.8903✅ Tokenizer saved after epoch 3\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 203ms/step - accuracy: 0.7305 - loss: 1.8904 - learning_rate: 0.0020\n",
            "Epoch 4/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.7308 - loss: 1.8424✅ Tokenizer saved after epoch 4\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 194ms/step - accuracy: 0.7308 - loss: 1.8423 - learning_rate: 0.0020\n",
            "Epoch 5/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.7358 - loss: 1.7537✅ Tokenizer saved after epoch 5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 198ms/step - accuracy: 0.7358 - loss: 1.7539 - learning_rate: 0.0020\n",
            "Epoch 6/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.7323 - loss: 1.7267✅ Tokenizer saved after epoch 6\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - accuracy: 0.7323 - loss: 1.7266 - learning_rate: 0.0020\n",
            "Epoch 7/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7373 - loss: 1.6462✅ Tokenizer saved after epoch 7\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 203ms/step - accuracy: 0.7372 - loss: 1.6463 - learning_rate: 0.0020\n",
            "Epoch 8/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7362 - loss: 1.6045✅ Tokenizer saved after epoch 8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 197ms/step - accuracy: 0.7363 - loss: 1.6045 - learning_rate: 0.0020\n",
            "Epoch 9/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7407 - loss: 1.5348✅ Tokenizer saved after epoch 9\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 197ms/step - accuracy: 0.7407 - loss: 1.5348 - learning_rate: 0.0020\n",
            "Epoch 10/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.7430 - loss: 1.4706✅ Tokenizer saved after epoch 10\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 214ms/step - accuracy: 0.7430 - loss: 1.4706 - learning_rate: 0.0020\n",
            "Epoch 11/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7487 - loss: 1.4031✅ Tokenizer saved after epoch 11\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 201ms/step - accuracy: 0.7487 - loss: 1.4031 - learning_rate: 0.0020\n",
            "Epoch 12/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.7544 - loss: 1.3373✅ Tokenizer saved after epoch 12\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 195ms/step - accuracy: 0.7544 - loss: 1.3374 - learning_rate: 0.0020\n",
            "Epoch 13/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.7595 - loss: 1.2842✅ Tokenizer saved after epoch 13\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 195ms/step - accuracy: 0.7595 - loss: 1.2842 - learning_rate: 0.0020\n",
            "Epoch 14/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.7650 - loss: 1.2344✅ Tokenizer saved after epoch 14\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 205ms/step - accuracy: 0.7650 - loss: 1.2344 - learning_rate: 0.0020\n",
            "Epoch 15/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.7722 - loss: 1.1860✅ Tokenizer saved after epoch 15\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 204ms/step - accuracy: 0.7722 - loss: 1.1861 - learning_rate: 0.0020\n",
            "Epoch 16/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.7784 - loss: 1.1325✅ Tokenizer saved after epoch 16\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 200ms/step - accuracy: 0.7784 - loss: 1.1326 - learning_rate: 0.0020\n",
            "Epoch 17/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7845 - loss: 1.0911✅ Tokenizer saved after epoch 17\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 197ms/step - accuracy: 0.7845 - loss: 1.0912 - learning_rate: 0.0020\n",
            "Epoch 18/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7878 - loss: 1.0616✅ Tokenizer saved after epoch 18\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 204ms/step - accuracy: 0.7878 - loss: 1.0617 - learning_rate: 0.0020\n",
            "Epoch 19/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7963 - loss: 1.0129✅ Tokenizer saved after epoch 19\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 197ms/step - accuracy: 0.7962 - loss: 1.0131 - learning_rate: 0.0020\n",
            "Epoch 20/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.7988 - loss: 0.9959✅ Tokenizer saved after epoch 20\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - accuracy: 0.7988 - loss: 0.9960 - learning_rate: 0.0020\n",
            "Epoch 21/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8037 - loss: 0.9639✅ Tokenizer saved after epoch 21\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - accuracy: 0.8036 - loss: 0.9640 - learning_rate: 0.0020\n",
            "Epoch 22/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8043 - loss: 0.9528✅ Tokenizer saved after epoch 22\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - accuracy: 0.8043 - loss: 0.9528 - learning_rate: 0.0020\n",
            "Epoch 23/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8103 - loss: 0.9216✅ Tokenizer saved after epoch 23\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 0.8103 - loss: 0.9217 - learning_rate: 0.0020\n",
            "Epoch 24/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8146 - loss: 0.8949✅ Tokenizer saved after epoch 24\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 204ms/step - accuracy: 0.8146 - loss: 0.8950 - learning_rate: 0.0020\n",
            "Epoch 25/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.8156 - loss: 0.8884✅ Tokenizer saved after epoch 25\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - accuracy: 0.8156 - loss: 0.8884 - learning_rate: 0.0020\n",
            "Epoch 26/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8199 - loss: 0.8687✅ Tokenizer saved after epoch 26\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 195ms/step - accuracy: 0.8199 - loss: 0.8688 - learning_rate: 0.0020\n",
            "Epoch 27/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8241 - loss: 0.8429✅ Tokenizer saved after epoch 27\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - accuracy: 0.8241 - loss: 0.8430 - learning_rate: 0.0020\n",
            "Epoch 28/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8256 - loss: 0.8368✅ Tokenizer saved after epoch 28\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 204ms/step - accuracy: 0.8256 - loss: 0.8368 - learning_rate: 0.0020\n",
            "Epoch 29/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8288 - loss: 0.8236✅ Tokenizer saved after epoch 29\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 211ms/step - accuracy: 0.8287 - loss: 0.8237 - learning_rate: 0.0020\n",
            "Epoch 30/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8301 - loss: 0.8114✅ Tokenizer saved after epoch 30\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 195ms/step - accuracy: 0.8301 - loss: 0.8114 - learning_rate: 0.0020\n",
            "Epoch 31/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8311 - loss: 0.8078✅ Tokenizer saved after epoch 31\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 197ms/step - accuracy: 0.8310 - loss: 0.8078 - learning_rate: 0.0020\n",
            "Epoch 32/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8336 - loss: 0.7890✅ Tokenizer saved after epoch 32\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - accuracy: 0.8336 - loss: 0.7890 - learning_rate: 0.0020\n",
            "Epoch 33/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8362 - loss: 0.7816✅ Tokenizer saved after epoch 33\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 195ms/step - accuracy: 0.8362 - loss: 0.7816 - learning_rate: 0.0020\n",
            "Epoch 34/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.8371 - loss: 0.7672✅ Tokenizer saved after epoch 34\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 196ms/step - accuracy: 0.8371 - loss: 0.7673 - learning_rate: 0.0020\n",
            "Epoch 35/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8397 - loss: 0.7585✅ Tokenizer saved after epoch 35\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - accuracy: 0.8397 - loss: 0.7586 - learning_rate: 0.0020\n",
            "Epoch 36/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8401 - loss: 0.7600✅ Tokenizer saved after epoch 36\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 211ms/step - accuracy: 0.8401 - loss: 0.7600 - learning_rate: 0.0020\n",
            "Epoch 37/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8396 - loss: 0.7592✅ Tokenizer saved after epoch 37\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 203ms/step - accuracy: 0.8396 - loss: 0.7592 - learning_rate: 0.0020\n",
            "Epoch 38/100\n",
            "\u001b[1m 22/157\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 230ms/step - accuracy: 0.8482 - loss: 0.7289"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zAZ0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/dataset_refined.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_sequence_length = 30\n",
        "\n",
        "# Tokenizer\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # Handle unknown words\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_train = np.array(response_sequences, dtype=np.int32)\n",
        "\n",
        "# **🧪 Train-Test Split**\n",
        "X_train = input_sequences\n",
        "\n",
        "# **🛠️ LSTM Model Architecture with Attention Layer**\n",
        "embedding_dim = 256\n",
        "lstm_units = 512  # Increased LSTM units for more capacity\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(embedding)\n",
        "\n",
        "# Attention Layer (using LSTM output as both query and value)\n",
        "attention = tf.keras.layers.Attention()([lstm_output, lstm_output])\n",
        "\n",
        "dropout = tf.keras.layers.Dropout(0.5)(attention)\n",
        "output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax', dtype='float32'))(dropout)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# **⚙️ Compile Model with Learning Rate Schedule**\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.002,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "# **⚙️ Compile Model with Learning Rate Schedule**\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),  # Use the lr_schedule directly here\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# **📌 Custom Callback to Save Tokenizer**\n",
        "class SaveTokenizerCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ Tokenizer saved after epoch {epoch + 1}\")\n",
        "\n",
        "# **🛡️ Callbacks**\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best.keras\")\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001),\n",
        "    SaveTokenizerCallback()\n",
        "]\n",
        "\n",
        "# **🎯 Train Model**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # Increased epochs for better training\n",
        "    batch_size=64,  # Larger batch size for stability\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **✅ Save Final Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Model saved as {final_model_path}\")\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.3):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = [np.random.choice(len(seq), p=seq) for seq in predicted_seq]\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([sampled_indices])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "pVIKdpoWrJDz",
        "outputId": "6610e349-ad2c-4eff-d3c4-97319f72d336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Vocabulary Size: 3665\n",
            "X_train shape: (8000, 30), y_train shape: (8000,)\n",
            "X_test shape: (2000, 30), y_test shape: (2000,)\n",
            "Epoch 1/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9249 - loss: 1.3369✅ Model and Tokenizer saved after epoch 1 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - accuracy: 0.9259 - loss: 1.3213 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0020\n",
            "Epoch 2/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0000e+00✅ Model and Tokenizer saved after epoch 2 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0020\n",
            "Epoch 3/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0000e+00✅ Model and Tokenizer saved after epoch 3 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0020\n",
            "Epoch 4/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0000e+00✅ Model and Tokenizer saved after epoch 4 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0020\n",
            "Epoch 5/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0000e+00✅ Model and Tokenizer saved after epoch 5 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 0.0020\n",
            "Epoch 6/100\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0000e+00✅ Model and Tokenizer saved after epoch 6 at /content/drive/MyDrive/chatbot/skyrim_chatbot_latest.keras and /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 4.0000e-04\n",
            "Epoch 7/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a3f9d7f61baf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# **Train Model**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zAZ0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/dataset_refined.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_sequence_length = 30\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # Handle unknown words\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_data = np.array([seq[-1] for seq in response_sequences], dtype=np.int32)  # Take only the last token\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# **📥 Load Pre-trained GloVe Embeddings**\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "glove_path = \"/content/drive/MyDrive/chatbot/glove.6B.300d.txt\"\n",
        "embedding_dict = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vectors = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_dict[word] = vectors\n",
        "\n",
        "# Fill embedding matrix\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if word in embedding_dict:\n",
        "        embedding_matrix[index] = embedding_dict[word]\n",
        "\n",
        "# **🛠️ LSTM Model Architecture with Bahdanau Attention**\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                            output_dim=embedding_dim,\n",
        "                                            weights=[embedding_matrix],\n",
        "                                            trainable=False)\n",
        "\n",
        "# Define Attention Mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Define Model\n",
        "inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True, dropout=0.3))(embedding)\n",
        "query = tf.keras.layers.LSTM(512, return_sequences=False)(lstm)\n",
        "\n",
        "attention = BahdanauAttention(512)\n",
        "context_vector, _ = attention(query, lstm)\n",
        "\n",
        "dense = tf.keras.layers.Dense(512, activation='relu')(context_vector)\n",
        "outputs = tf.keras.layers.Dense(vocab_size, activation='softmax', dtype='float32')(dense)\n",
        "\n",
        "model = tf.keras.models.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Best model path (saving multiple checkpoints)\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_ea.keras\")\n",
        "\n",
        "# Define a custom callback to save the model and tokenizer after each epoch\n",
        "class SaveModelAndTokenizerCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save tokenizer\n",
        "        tokenizer_path = os.path.join(drive_save_path, f\"tokenizer.pkl\")\n",
        "        with open(tokenizer_path, \"wb\") as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        # Save model\n",
        "        epoch_model_path = os.path.join(drive_save_path, f\"skyrim_chatbot_latest.keras\")\n",
        "        self.model.save(epoch_model_path)\n",
        "\n",
        "        print(f\"✅ Model and Tokenizer saved after epoch {epoch+1} at {epoch_model_path} and {tokenizer_path}\")\n",
        "\n",
        "# Callbacks including tokenizer and model saving\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "    ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=False),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.0001),\n",
        "    SaveModelAndTokenizerCallback()  # Custom callback to save model and tokenizer\n",
        "]\n",
        "\n",
        "\n",
        "# **Train Model**\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# **Save Final Model & Tokenizer**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "\n",
        "final_tokenizer_path = os.path.join(drive_save_path, \"tokenizer_final.pkl\")\n",
        "with open(final_tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(f\"✅ Final Model saved at {final_model_path}\")\n",
        "print(f\"✅ Final Tokenizer saved at {final_tokenizer_path}\")\n",
        "\n",
        "\n",
        "# **✅ Save Final Model**\n",
        "final_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_final.keras\")\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Model saved as {final_model_path}\")\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.3):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = np.random.choice(len(predicted_seq), p=predicted_seq)\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([[sampled_indices]])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import random\n",
        "from google.colab import drive  # Mount Google Drive\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# **📁 Define Google Drive Path**\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "model_path = os.path.join(drive_save_path, \"skyrim_chatbot_ea.keras\")\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "\n",
        "# **📌 Define Custom Bahdanau Attention Layer (Fixed)**\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(BahdanauAttention, self).__init__(**kwargs)\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# **📌 Load the Updated Model**\n",
        "if os.path.exists(model_path):\n",
        "    model = tf.keras.models.load_model(model_path, custom_objects={\"BahdanauAttention\": BahdanauAttention})\n",
        "    print(f\"✅ Model loaded from {model_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Model file not found at {model_path}\")\n",
        "\n",
        "# **📌 Load the Tokenizer**\n",
        "if os.path.exists(tokenizer_path):\n",
        "    with open(tokenizer_path, \"rb\") as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    print(f\"✅ Tokenizer loaded from {tokenizer_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"❌ Tokenizer file not found at {tokenizer_path}\")\n",
        "\n",
        "def generate_response(input_text, max_response_length=30, temperature=0.2, top_k=5):\n",
        "    max_sequence_length = 30  # Match training sequence length\n",
        "\n",
        "    # **Tokenize and pad input**\n",
        "    input_seq = pad_sequences(tokenizer.texts_to_sequences([input_text]), maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # **Predict output probabilities**\n",
        "    prediction = model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # **🔹 Fix: Reshape Output If Needed**\n",
        "    if len(prediction.shape) == 2 and prediction.shape[1] == len(tokenizer.word_index) + 1:\n",
        "        prediction = prediction.reshape((1, 1, prediction.shape[1]))\n",
        "\n",
        "    # **Validate Shape**\n",
        "    if len(prediction.shape) != 3:\n",
        "        raise ValueError(f\"Unexpected prediction shape: {prediction.shape}. Expected (1, sequence_length, vocab_size).\")\n",
        "\n",
        "    response_text = []\n",
        "    previous_words = set()\n",
        "\n",
        "    for i in range(min(max_response_length, prediction.shape[1])):\n",
        "        word_probs = prediction[0, i, :]\n",
        "\n",
        "        # **Temperature Scaling**\n",
        "        word_probs = np.exp(word_probs / temperature)\n",
        "        word_probs /= np.sum(word_probs)\n",
        "\n",
        "        # **Select Top-k Words**\n",
        "        top_indices = np.argsort(word_probs)[-top_k:]\n",
        "        if len(top_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        word_idx = np.random.choice(top_indices, p=word_probs[top_indices] / np.sum(word_probs[top_indices]))\n",
        "\n",
        "        if word_idx not in tokenizer.index_word:\n",
        "            continue\n",
        "\n",
        "        word = tokenizer.index_word[word_idx]\n",
        "\n",
        "        if word in previous_words or word in [\"<OOV>\", \"<UNK>\", \"<PAD>\", \"<END>\"]:\n",
        "            continue\n",
        "\n",
        "        response_text.append(word)\n",
        "        previous_words.add(word)\n",
        "\n",
        "        if word in [\".\", \"?\", \"!\"]:\n",
        "            break\n",
        "\n",
        "    response = \" \".join(response_text).strip()\n",
        "    return response if response else \"I am not sure what you mean, traveler.\"\n",
        "\n",
        "# **Chatbot interaction loop**\n",
        "try:\n",
        "    print(\"Welcome to the Elder Scrolls NPC Chatbot! Type 'exit' or 'quit' to end the conversation.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip().lower()\n",
        "\n",
        "        # **Exit conditions**\n",
        "        if user_input in [\"exit\", \"quit\"]:\n",
        "            print(\"NPC: Farewell, traveler. May the blessings of the Tribunal be with you!\")\n",
        "            break\n",
        "\n",
        "        # **Generate and display NPC response**\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"NPC: {response}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nNPC: Farewell, traveler. May the blessings of the Tribunal be with you!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej0qvt-MTnnN",
        "outputId": "f4420c82-6b4e-4b09-ac87-31fded2c9d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'bahdanau_attention_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded from /content/drive/MyDrive/chatbot/skyrim_chatbot_ea.keras\n",
            "✅ Tokenizer loaded from /content/drive/MyDrive/chatbot/tokenizer.pkl\n",
            "Welcome to the Elder Scrolls NPC Chatbot! Type 'exit' or 'quit' to end the conversation.\n",
            "You: hi\n",
            "NPC: I am not sure what you mean, traveler.\n",
            "You: hello\n",
            "NPC: I am not sure what you mean, traveler.\n",
            "You: fuck off\n",
            "NPC: I am not sure what you mean, traveler.\n",
            "You: quit\n",
            "NPC: Farewell, traveler. May the blessings of the Tribunal be with you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# **🔗 Mount Google Drive**\n",
        "drive.mount('/content/drive')\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "\n",
        "# **⚡ Enable mixed precision**\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# **📝 Text Preprocessing Function**\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# **📥 Load Skyrim Dialogue Dataset**\n",
        "def load_skyrim_dialogue_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/dataset_refined.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "# **📌 Tokenization & Padding**\n",
        "max_sequence_length = 30\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # Handle unknown words\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"✅ Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer_path = os.path.join(drive_save_path, \"tokenizer.pkl\")\n",
        "with open(tokenizer_path, \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post')\n",
        "y_data = np.array([seq[-1] for seq in response_sequences], dtype=np.int32)  # Take only the last token\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_sequences, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "# **📥 Load Pre-trained GloVe Embeddings**\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "glove_path = \"/content/drive/MyDrive/chatbot/glove.6B.300d.txt\"\n",
        "embedding_dict = {}\n",
        "with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vectors = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_dict[word] = vectors\n",
        "\n",
        "# Fill embedding matrix\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if word in embedding_dict:\n",
        "        embedding_matrix[index] = embedding_dict[word]\n",
        "\n",
        "# **🛠️ LSTM Model Architecture with Bahdanau Attention**\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                            output_dim=embedding_dim,\n",
        "                                            weights=[embedding_matrix],\n",
        "                                            trainable=False)\n",
        "\n",
        "# Define Attention Mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# **🗣️ Generate NPC Response Function**\n",
        "def generate_response(player_input, model, tokenizer, max_sequence_length, temperature=0.3):\n",
        "    input_seq = tokenizer.texts_to_sequences([player_input])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    predicted_seq = model.predict(input_seq)[0]\n",
        "    predicted_seq = np.log(predicted_seq + 1e-8) / temperature\n",
        "    predicted_seq = np.exp(predicted_seq) / np.sum(np.exp(predicted_seq))\n",
        "    sampled_indices = np.random.choice(len(predicted_seq), p=predicted_seq)\n",
        "\n",
        "    response = tokenizer.sequences_to_texts([[sampled_indices]])[0]\n",
        "    return response if response else \"I am not sure, traveler.\"\n",
        "\n",
        "# **🤖 Example Chatbot Interaction**\n",
        "print(\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"quit\"]:\n",
        "        print(\"NPC: Farewell, traveler!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input, model, tokenizer, max_sequence_length)\n",
        "    print(f\"NPC: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "u10tmeTLVL5j",
        "outputId": "a1dc3ebc-176f-425b-a5e3-f355a8306d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Vocabulary Size: 3665\n",
            "X_train shape: (8000, 30), y_train shape: (8000,)\n",
            "X_test shape: (2000, 30), y_test shape: (2000,)\n",
            "Welcome to the Skyrim Chatbot! Type 'exit' to leave.\n",
            "You: HELLO\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step\n",
            "NPC: <OOV>\n",
            "You: HI\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "NPC: <OOV>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cb34de74cb18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Welcome to the Skyrim Chatbot! Type 'exit' to leave.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NPC: Farewell, traveler!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0XzQED44pIj",
        "outputId": "85483b20-8cd5-491e-fd14-f4a935483315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequences shape: (10000, 30)\n",
            "Response sequences shape: (10000, 30)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Input sequences shape: {input_sequences.shape}\")\n",
        "print(f\"Response sequences shape: {response_sequences.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_save_path = \"/content/drive/MyDrive/chatbot/\"\n",
        "os.makedirs(drive_save_path, exist_ok=True)\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower().strip()\n",
        "    contractions = {\n",
        "        r\"n't\": \" not\", r\"'re\": \" are\", r\"'s\": \" is\",\n",
        "        r\"'d\": \" would\", r\"'ll\": \" will\", r\"'t\": \" not\",\n",
        "        r\"'ve\": \" have\", r\"'m\": \" am\", r\"what's\": \"what is\",\n",
        "        r\"that's\": \"that is\", r\"there's\": \"there is\"\n",
        "    }\n",
        "    for pat, repl in contractions.items():\n",
        "        text = re.sub(pat, repl, text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def synonym_replacement(text, n=1):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    random.shuffle(random_word_list)\n",
        "    for _ in range(n):\n",
        "        for word in random_word_list:\n",
        "            synonyms = [lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas()]\n",
        "            if synonyms:\n",
        "                synonym = random.choice(synonyms)\n",
        "                new_words = [synonym if w == word else w for w in new_words]\n",
        "                break\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def load_skyrim_dialogue_dataset(file_path, augment_data=True):\n",
        "    df = pd.read_csv(file_path)\n",
        "    player_inputs = df['Player Input'].astype(str).apply(preprocess_text).tolist()\n",
        "    npc_responses = df['NPC Response'].astype(str).apply(preprocess_text).tolist()\n",
        "    if augment_data:\n",
        "        augmented_inputs, augmented_responses = [], []\n",
        "        for inp, resp in zip(player_inputs, npc_responses):\n",
        "            augmented_inputs.append(inp)\n",
        "            augmented_responses.append(resp)\n",
        "            if len(inp.split()) > 3 and len(resp.split()) > 3:\n",
        "                augmented_inputs.append(synonym_replacement(inp))\n",
        "                augmented_responses.append(synonym_replacement(resp))\n",
        "        return augmented_inputs, augmented_responses\n",
        "    return player_inputs, npc_responses\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/chatbot/dataset_refined.csv\"\n",
        "player_inputs, npc_responses = load_skyrim_dialogue_dataset(file_path)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='')\n",
        "tokenizer.fit_on_texts(player_inputs + npc_responses)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_sequence_length = 40\n",
        "\n",
        "input_sequences = pad_sequences(tokenizer.texts_to_sequences(player_inputs), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "response_sequences = pad_sequences(tokenizer.texts_to_sequences(npc_responses), maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "X = np.array(input_sequences)\n",
        "y = np.array(response_sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, lstm_units, max_sequence_length):\n",
        "    encoder_inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
        "    encoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "    encoder_lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=False, return_state=True, dropout=0.2, recurrent_dropout=0.1)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "    decoder_inputs = tf.keras.layers.Input(shape=(max_sequence_length,))\n",
        "    decoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.1)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "    decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "    outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    return tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "\n",
        "embedding_dim, lstm_units, learning_rate = 300, 384, 0.001\n",
        "model = build_model(vocab_size, embedding_dim, lstm_units, max_sequence_length)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1.0), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "class ModelAndTokenizerCheckpoint(Callback):\n",
        "    def __init__(self, save_dir, tokenizer):\n",
        "        super().__init__()\n",
        "        self.save_dir = save_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        model.save(os.path.join(self.save_dir, f\"model_DE.keras\"))\n",
        "        with open(os.path.join(self.save_dir, f\"tokenizer_DE.pkl\"), \"wb\") as handle:\n",
        "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "best_model_path = os.path.join(drive_save_path, \"skyrim_chatbot_best_DE.keras\")\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True), ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_loss'), ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5), ModelAndTokenizerCheckpoint(drive_save_path, tokenizer)]\n",
        "\n",
        "history = model.fit([X_train, y_train], y_train, epochs=100, batch_size=256, validation_data=([X_test, y_test], y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "model.save(os.path.join(drive_save_path, \"skyrim_chatbot_final_DE.keras\"))\n",
        "with open(os.path.join(drive_save_path, \"tokenizer_final_DE.pkl\"), \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def generate_response(input_text, model, tokenizer, max_length):\n",
        "    input_seq = pad_sequences(tokenizer.texts_to_sequences([input_text]), maxlen=max_length, padding='post')\n",
        "    decoder_input = np.zeros((1, max_length))\n",
        "    response = []\n",
        "    for _ in range(max_length):\n",
        "        output = model.predict([input_seq, decoder_input])\n",
        "        predicted_id = np.argmax(output[0, -1, :])\n",
        "        response.append(predicted_id)\n",
        "        decoder_input[0, len(response) - 1] = predicted_id\n",
        "        if predicted_id == tokenizer.word_index['<OOV>']:\n",
        "            break\n",
        "    return tokenizer.sequences_to_texts([response])[0]\n",
        "\n",
        "def run_chatbot():\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            break\n",
        "        print(f\"NPC: {generate_response(user_input, model, tokenizer, max_sequence_length)}\")\n",
        "\n",
        "run_chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "SqdAkounpabo",
        "outputId": "467edb87-9d5b-466b-dc8a-1667c253579c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 40, 384), dtype=float16)\n  • training=True\n  • mask=tf.Tensor(shape=(None, 40), dtype=bool)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-13073978eb3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelAndTokenizerCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"skyrim_chatbot_final_DE.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling TimeDistributed.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TimeDistributed.call():\n  • inputs=tf.Tensor(shape=(None, 40, 384), dtype=float16)\n  • training=True\n  • mask=tf.Tensor(shape=(None, 40), dtype=bool)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BfSC0_TXrK_l",
        "outputId": "ba061809-f522-476b-cdf8-67bb447353b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "0e4e08776b2945ec9c99672bf74dd27e"
            }
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}